{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wan 2.1 Video Generation on Kaggle\n",
        "\n",
        "This notebook generates videos from Mahabharata stories using Wan 2.1 model.\n",
        "\n",
        "**Workflow:**\n",
        "1. Create Vector Database from PDF (first time only)\n",
        "2. Retrieve passages from vector DB\n",
        "3. Generate story scripts (~5 scenes) and narration\n",
        "4. Generate videos from scripts\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. **Add GitHub as Data Source**:\n",
        "   - Click \"Add Data\" → \"GitHub\"\n",
        "   - Enter your repository URL: `https://github.com/VibhavVPandit/Mahabharata`\n",
        "\n",
        "2. **Upload PDF**:\n",
        "   - Upload `Mahabharata.pdf` to `/kaggle/input/`\n",
        "\n",
        "3. **Configure Notebook Settings** (in sidebar):\n",
        "   - **Accelerator**: GPU T4 x2\n",
        "   - **Language**: Python 3\n",
        "   - **Internet**: On\n",
        "   - **Persistence**: Files Only (optional)\n",
        "\n",
        "4. **Set API Key**:\n",
        "   - Go to Settings → Secrets → Add Secret\n",
        "   - Name: `GEMINI_API_KEY`\n",
        "   - Value: Your Gemini API key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository (if not already added as data source)\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# If repository is added as data source, it will be in /kaggle/input/\n",
        "# Otherwise, clone it here\n",
        "repo_path = Path(\"/kaggle/working/repo\")\n",
        "if not repo_path.exists():\n",
        "    # Update with your repository URL\n",
        "    # !git clone <YOUR_REPO_URL> /kaggle/working/repo\n",
        "    repo_path = Path(\"/kaggle/working/repo\")\n",
        "else:\n",
        "    # Find repo in input directory\n",
        "    input_dir = Path(\"/kaggle/input\")\n",
        "    for item in input_dir.iterdir():\n",
        "        if item.is_dir() and (item / \"kaggle\").exists():\n",
        "            repo_path = item\n",
        "            break\n",
        "\n",
        "print(f\"Repository path: {repo_path}\")\n",
        "\n",
        "# Add kaggle directory to path\n",
        "kaggle_dir = repo_path / \"kaggle\"\n",
        "if not kaggle_dir.exists():\n",
        "    # Try alternative location\n",
        "    kaggle_dir = Path(\"/kaggle/input\") / \"kaggle\"\n",
        "    if not kaggle_dir.exists():\n",
        "        kaggle_dir = Path(\"/kaggle/working\") / \"kaggle\"\n",
        "\n",
        "sys.path.insert(0, str(kaggle_dir.parent))\n",
        "sys.path.insert(0, str(kaggle_dir))\n",
        "print(f\"Kaggle directory: {kaggle_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q diffusers>=0.21.0 transformers>=4.30.0 accelerate>=0.20.0\n",
        "!pip install -q imageio>=2.31.0 imageio-ffmpeg>=0.4.8 sentencepiece\n",
        "!pip install -q chromadb>=0.4.15 sentence-transformers>=2.2.0\n",
        "!pip install -q google-generativeai>=0.3.0 pydantic>=2.0.0 pyyaml>=6.0.1 numpy>=1.24.0\n",
        "!pip install -q pypdf>=3.0.0\n",
        "\n",
        "print(\"✓ Dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Hugging Face cache in /kaggle/tmp/\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "cache_dir = Path(\"/kaggle/tmp/huggingface\")\n",
        "cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "os.environ['HF_HOME'] = str(cache_dir)\n",
        "os.environ['TRANSFORMERS_CACHE'] = str(cache_dir)\n",
        "os.environ['HF_DATASETS_CACHE'] = str(cache_dir)\n",
        "\n",
        "print(f\"✓ Hugging Face cache set to: {cache_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "QUERY = None  # Optional: \"Arjuna\" or \"Kurukshetra\" for targeted retrieval, None for random\n",
        "\n",
        "# PDF file path (upload Mahabharata.pdf to /kaggle/input/)\n",
        "PDF_PATH = \"/kaggle/input/Mahabharata.pdf\"  # Update if your PDF has different name/path\n",
        "\n",
        "print(f\"Query: {QUERY if QUERY else 'Random (will retrieve random passage)'}\")\n",
        "print(f\"PDF Path: {PDF_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Vector Database from PDF\n",
        "\n",
        "**Run this section first** to create the vector database from the Mahabharata PDF.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Vector Database from PDF\n",
        "import sys\n",
        "import re\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional, Set, Tuple\n",
        "from collections import Counter\n",
        "\n",
        "sys.path.insert(0, str(kaggle_dir))\n",
        "from story_pipeline import KaggleVectorStore, KaggleEmbedder, Passage\n",
        "\n",
        "# ============================================================================\n",
        "# TEXT CLEANING FUNCTIONS (Preserves Paragraph Structure)\n",
        "# ============================================================================\n",
        "\n",
        "def normalize_unicode(text: str) -> str:\n",
        "    \"\"\"Normalize Unicode characters (NFD to NFC).\"\"\"\n",
        "    return unicodedata.normalize('NFC', text)\n",
        "\n",
        "def remove_pdf_artifacts(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove common PDF artifacts:\n",
        "    - Page numbers (standalone numbers, \"Page X\", etc.)\n",
        "    - Repeated headers/footers (lines that appear many times)\n",
        "    - Common PDF metadata patterns\n",
        "    \"\"\"\n",
        "    lines = text.split('\\n')\n",
        "    \n",
        "    # Remove standalone page numbers (1-4 digits at start/end of line)\n",
        "    lines = [re.sub(r'^\\s*\\d{1,4}\\s*$', '', line) for line in lines]\n",
        "    \n",
        "    # Remove \"Page X\" patterns\n",
        "    lines = [re.sub(r'^\\s*Page\\s+\\d+\\s*$', '', line, flags=re.IGNORECASE) for line in lines]\n",
        "    \n",
        "    # Remove common PDF metadata patterns\n",
        "    lines = [re.sub(r'^\\s*Table of Contents\\s*$', '', line, flags=re.IGNORECASE) for line in lines]\n",
        "    \n",
        "    # Count line frequencies to detect repeated headers/footers\n",
        "    line_counts = Counter(line.strip() for line in lines if line.strip())\n",
        "    \n",
        "    # Remove lines that appear more than 10 times (likely headers/footers)\n",
        "    threshold = 10\n",
        "    lines = [\n",
        "        line for line in lines \n",
        "        if not line.strip() or line_counts.get(line.strip(), 0) <= threshold\n",
        "    ]\n",
        "    \n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "def fix_hyphenation(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Fix hyphenation at line breaks in PDFs.\n",
        "    Example: \"Ar-\\njuna\" -> \"Arjuna\"\n",
        "    \"\"\"\n",
        "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
        "    return text\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Improved text cleaning that PRESERVES paragraph structure.\n",
        "    \n",
        "    Key difference: Preserves double newlines as paragraph markers.\n",
        "    \"\"\"\n",
        "    print(\"  Step 1: Normalizing Unicode...\")\n",
        "    text = normalize_unicode(text)\n",
        "    \n",
        "    print(\"  Step 2: Removing PDF artifacts...\")\n",
        "    text = remove_pdf_artifacts(text)\n",
        "    \n",
        "    print(\"  Step 3: Fixing hyphenation...\")\n",
        "    text = fix_hyphenation(text)\n",
        "    \n",
        "    print(\"  Step 4: Normalizing whitespace (preserving paragraphs)...\")\n",
        "    # Mark paragraph breaks before collapsing whitespace\n",
        "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n<PARA_BREAK>\\n\\n', text)\n",
        "    # Collapse single newlines and multiple spaces\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    text = re.sub(r' ?\\n ?', ' ', text)\n",
        "    # Restore paragraph breaks\n",
        "    text = text.replace('<PARA_BREAK>', '\\n\\n')\n",
        "    \n",
        "    print(\"  Step 5: Removing problematic characters...\")\n",
        "    # Remove control characters\n",
        "    text = re.sub(r'[\\x00-\\x08\\x0B-\\x1F\\x7F-\\x9F]', '', text)\n",
        "    # Keep more punctuation\n",
        "    text = re.sub(r'[^\\w\\s.,!?;:\\-\\(\\)\\[\\]\\'\"—–\\n]', ' ', text)\n",
        "    \n",
        "    print(\"  Step 6: Normalizing quotes...\")\n",
        "    text = text.replace('\"', '\"').replace('\"', '\"')\n",
        "    text = text.replace(''', \"'\").replace(''', \"'\")\n",
        "    \n",
        "    # Final cleanup\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Max 2 newlines\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "# ============================================================================\n",
        "# SENTENCE TOKENIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def split_into_sentences(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text into sentences using regex-based approach.\n",
        "    Handles common abbreviations and edge cases.\n",
        "    \"\"\"\n",
        "    # Common abbreviations that shouldn't end sentences\n",
        "    abbreviations = r'(?:Mr|Mrs|Ms|Dr|Prof|Sr|Jr|vs|etc|viz|al|eg|ie|cf|Vol|Ch|Sec|Fig|No|ca|Inc|Ltd|Corp|St|Ave|Blvd|Rd)'\n",
        "    \n",
        "    # Pattern for sentence boundaries\n",
        "    # Matches: period/question/exclamation followed by space and capital letter\n",
        "    # But not after abbreviations\n",
        "    sentence_pattern = rf'(?<!{abbreviations})(?<=[.!?])\\s+(?=[A-Z])'\n",
        "    \n",
        "    sentences = re.split(sentence_pattern, text)\n",
        "    \n",
        "    # Clean up and filter empty sentences\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    \n",
        "    return sentences\n",
        "\n",
        "# ============================================================================\n",
        "# HYBRID SEMANTIC + PARAGRAPH-AWARE CHUNKING\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class TextChunk:\n",
        "    \"\"\"Represents a chunk of text with metadata.\"\"\"\n",
        "    text: str\n",
        "    chunk_id: str\n",
        "    characters: List[str]\n",
        "    themes: List[str]\n",
        "    chapter: Optional[str] = None\n",
        "    section: Optional[str] = None\n",
        "    embedding: Optional[np.ndarray] = field(default=None, repr=False)\n",
        "\n",
        "# Chunking configuration\n",
        "CHUNK_CONFIG = {\n",
        "    'target_size': 400,      # Target chunk size in characters\n",
        "    'min_size': 100,         # Minimum chunk size\n",
        "    'max_size': 800,         # Maximum chunk size before forced split\n",
        "    'overlap_sentences': 1,  # Number of sentences to overlap\n",
        "    'semantic_threshold': 0.75,  # Cosine similarity threshold for merging\n",
        "}\n",
        "\n",
        "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
        "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
        "    if vec1 is None or vec2 is None:\n",
        "        return 0.0\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "    return np.dot(vec1, vec2) / (norm1 * norm2)\n",
        "\n",
        "def split_into_paragraphs(text: str) -> List[str]:\n",
        "    \"\"\"Split text into paragraphs (double newline separated).\"\"\"\n",
        "    paragraphs = re.split(r'\\n\\n+', text)\n",
        "    return [p.strip() for p in paragraphs if p.strip()]\n",
        "\n",
        "def recursive_split(text: str, max_size: int, separators: List[str] = None) -> List[str]:\n",
        "    \"\"\"\n",
        "    Recursively split text using hierarchy of separators.\n",
        "    Tries to split on paragraph breaks first, then sentences, then words.\n",
        "    \"\"\"\n",
        "    if separators is None:\n",
        "        separators = ['\\n\\n', '. ', '! ', '? ', ', ', ' ']\n",
        "    \n",
        "    if len(text) <= max_size:\n",
        "        return [text]\n",
        "    \n",
        "    # Try each separator in order\n",
        "    for sep in separators:\n",
        "        if sep in text:\n",
        "            parts = text.split(sep)\n",
        "            \n",
        "            # Rebuild chunks respecting max_size\n",
        "            chunks = []\n",
        "            current = \"\"\n",
        "            \n",
        "            for i, part in enumerate(parts):\n",
        "                # Add separator back (except for last part)\n",
        "                part_with_sep = part + sep if i < len(parts) - 1 else part\n",
        "                \n",
        "                if len(current) + len(part_with_sep) <= max_size:\n",
        "                    current += part_with_sep\n",
        "                else:\n",
        "                    if current:\n",
        "                        chunks.append(current.strip())\n",
        "                    current = part_with_sep\n",
        "            \n",
        "            if current:\n",
        "                chunks.append(current.strip())\n",
        "            \n",
        "            # Recursively split any chunks that are still too large\n",
        "            result = []\n",
        "            remaining_seps = separators[separators.index(sep) + 1:] if sep in separators else []\n",
        "            for chunk in chunks:\n",
        "                if len(chunk) > max_size and remaining_seps:\n",
        "                    result.extend(recursive_split(chunk, max_size, remaining_seps))\n",
        "                else:\n",
        "                    result.append(chunk)\n",
        "            \n",
        "            return result\n",
        "    \n",
        "    # Fallback: hard split by character count\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), max_size):\n",
        "        chunks.append(text[i:i + max_size])\n",
        "    return chunks\n",
        "\n",
        "def paragraph_aware_chunking(text: str, config: dict = None) -> List[str]:\n",
        "    \"\"\"\n",
        "    Stage 1: Paragraph-aware recursive chunking.\n",
        "    \n",
        "    1. Split into paragraphs\n",
        "    2. Group small paragraphs together\n",
        "    3. Split large paragraphs using recursive approach\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = CHUNK_CONFIG\n",
        "    \n",
        "    target_size = config['target_size']\n",
        "    min_size = config['min_size']\n",
        "    max_size = config['max_size']\n",
        "    \n",
        "    paragraphs = split_into_paragraphs(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    \n",
        "    for para in paragraphs:\n",
        "        # If paragraph alone is too large, split it recursively\n",
        "        if len(para) > max_size:\n",
        "            # Save current chunk first\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                current_chunk = \"\"\n",
        "            \n",
        "            # Split large paragraph\n",
        "            sub_chunks = recursive_split(para, max_size)\n",
        "            chunks.extend(sub_chunks)\n",
        "        \n",
        "        # If adding paragraph keeps us under target, accumulate\n",
        "        elif len(current_chunk) + len(para) + 2 <= target_size:\n",
        "            current_chunk += (\"\\n\\n\" if current_chunk else \"\") + para\n",
        "        \n",
        "        # If current chunk is big enough, save it and start new one\n",
        "        elif len(current_chunk) >= min_size:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = para\n",
        "        \n",
        "        # Current chunk is small, but adding this makes it too big\n",
        "        else:\n",
        "            current_chunk += (\"\\n\\n\" if current_chunk else \"\") + para\n",
        "            if len(current_chunk) > max_size:\n",
        "                # Split the combined chunk\n",
        "                sub_chunks = recursive_split(current_chunk, max_size)\n",
        "                chunks.extend(sub_chunks[:-1])\n",
        "                current_chunk = sub_chunks[-1] if sub_chunks else \"\"\n",
        "    \n",
        "    # Don't forget the last chunk\n",
        "    if current_chunk.strip():\n",
        "        chunks.append(current_chunk.strip())\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "def semantic_chunk_refinement(\n",
        "    chunks: List[str], \n",
        "    embedder, \n",
        "    config: dict = None\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Stage 2: Semantic refinement using embeddings.\n",
        "    \n",
        "    1. Compute embeddings for all chunks\n",
        "    2. Merge adjacent chunks if semantically similar and combined size is acceptable\n",
        "    3. Split chunks at semantic boundaries if they contain topic shifts\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = CHUNK_CONFIG\n",
        "    \n",
        "    if not chunks:\n",
        "        return chunks\n",
        "    \n",
        "    threshold = config['semantic_threshold']\n",
        "    min_size = config['min_size']\n",
        "    max_size = config['max_size']\n",
        "    \n",
        "    print(f\"    Computing embeddings for {len(chunks)} chunks...\")\n",
        "    embeddings = embedder.embed(chunks)\n",
        "    \n",
        "    # Stage 2a: Merge semantically similar adjacent chunks\n",
        "    print(\"    Merging semantically similar chunks...\")\n",
        "    merged_chunks = []\n",
        "    merged_embeddings = []\n",
        "    i = 0\n",
        "    \n",
        "    while i < len(chunks):\n",
        "        current_chunk = chunks[i]\n",
        "        current_emb = embeddings[i]\n",
        "        \n",
        "        # Try to merge with next chunks\n",
        "        while i + 1 < len(chunks):\n",
        "            next_chunk = chunks[i + 1]\n",
        "            next_emb = embeddings[i + 1]\n",
        "            combined_len = len(current_chunk) + len(next_chunk) + 2\n",
        "            \n",
        "            # Check if merge is beneficial\n",
        "            similarity = cosine_similarity(current_emb, next_emb)\n",
        "            \n",
        "            # Merge if: similar content AND combined size is reasonable\n",
        "            if similarity > threshold and combined_len <= max_size:\n",
        "                current_chunk = current_chunk + \"\\n\\n\" + next_chunk\n",
        "                # Recompute embedding for merged chunk\n",
        "                current_emb = embedder.embed_single(current_chunk)\n",
        "                i += 1\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        merged_chunks.append(current_chunk)\n",
        "        merged_embeddings.append(current_emb)\n",
        "        i += 1\n",
        "    \n",
        "    print(f\"    After merging: {len(merged_chunks)} chunks\")\n",
        "    \n",
        "    # Stage 2b: Split chunks with internal topic shifts\n",
        "    print(\"    Checking for internal topic shifts...\")\n",
        "    final_chunks = []\n",
        "    \n",
        "    for chunk in merged_chunks:\n",
        "        # Only check chunks that are large enough to potentially split\n",
        "        if len(chunk) > config['target_size'] * 1.5:\n",
        "            sentences = split_into_sentences(chunk)\n",
        "            \n",
        "            if len(sentences) >= 4:  # Need enough sentences to detect shifts\n",
        "                # Compute embeddings for sentence groups (sliding window)\n",
        "                window_size = 2\n",
        "                shifts = []\n",
        "                \n",
        "                for j in range(len(sentences) - window_size * 2 + 1):\n",
        "                    group1 = ' '.join(sentences[j:j + window_size])\n",
        "                    group2 = ' '.join(sentences[j + window_size:j + window_size * 2])\n",
        "                    \n",
        "                    emb1 = embedder.embed_single(group1)\n",
        "                    emb2 = embedder.embed_single(group2)\n",
        "                    \n",
        "                    similarity = cosine_similarity(emb1, emb2)\n",
        "                    shifts.append((j + window_size, similarity))\n",
        "                \n",
        "                # Find significant topic shifts (low similarity points)\n",
        "                if shifts:\n",
        "                    min_sim_idx, min_sim = min(shifts, key=lambda x: x[1])\n",
        "                    \n",
        "                    # If there's a clear topic shift, split there\n",
        "                    if min_sim < threshold - 0.1:  # Significant drop in similarity\n",
        "                        split_point = min_sim_idx\n",
        "                        chunk1 = ' '.join(sentences[:split_point])\n",
        "                        chunk2 = ' '.join(sentences[split_point:])\n",
        "                        \n",
        "                        if len(chunk1) >= min_size and len(chunk2) >= min_size:\n",
        "                            final_chunks.append(chunk1)\n",
        "                            final_chunks.append(chunk2)\n",
        "                            continue\n",
        "            \n",
        "        final_chunks.append(chunk)\n",
        "    \n",
        "    print(f\"    After topic-shift splitting: {len(final_chunks)} chunks\")\n",
        "    return final_chunks\n",
        "\n",
        "def add_sentence_overlap(chunks: List[str], overlap_sentences: int = 1) -> List[str]:\n",
        "    \"\"\"\n",
        "    Add sentence overlap between chunks for better context continuity.\n",
        "    \"\"\"\n",
        "    if overlap_sentences <= 0 or len(chunks) <= 1:\n",
        "        return chunks\n",
        "    \n",
        "    overlapped_chunks = [chunks[0]]\n",
        "    \n",
        "    for i in range(1, len(chunks)):\n",
        "        # Get last N sentences from previous chunk\n",
        "        prev_sentences = split_into_sentences(chunks[i - 1])\n",
        "        overlap_text = ' '.join(prev_sentences[-overlap_sentences:]) if prev_sentences else \"\"\n",
        "        \n",
        "        # Prepend to current chunk (with marker for clarity)\n",
        "        if overlap_text:\n",
        "            overlapped_chunks.append(overlap_text + \" \" + chunks[i])\n",
        "        else:\n",
        "            overlapped_chunks.append(chunks[i])\n",
        "    \n",
        "    return overlapped_chunks\n",
        "\n",
        "def hybrid_semantic_chunking(\n",
        "    text: str, \n",
        "    embedder,\n",
        "    config: dict = None,\n",
        "    add_overlap: bool = True\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Main hybrid chunking function combining:\n",
        "    1. Paragraph-aware recursive splitting\n",
        "    2. Semantic refinement (merge similar, split at topic shifts)\n",
        "    3. Sentence overlap for context continuity\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = CHUNK_CONFIG\n",
        "    \n",
        "    print(\"  Stage 1: Paragraph-aware recursive chunking...\")\n",
        "    chunks = paragraph_aware_chunking(text, config)\n",
        "    print(f\"    Created {len(chunks)} initial chunks\")\n",
        "    \n",
        "    print(\"  Stage 2: Semantic refinement...\")\n",
        "    chunks = semantic_chunk_refinement(chunks, embedder, config)\n",
        "    \n",
        "    if add_overlap:\n",
        "        print(\"  Stage 3: Adding sentence overlap...\")\n",
        "        chunks = add_sentence_overlap(chunks, config['overlap_sentences'])\n",
        "        print(f\"    Final chunks with overlap: {len(chunks)}\")\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# ============================================================================\n",
        "# METADATA EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "MAHABHARAT_CHARACTERS = [\n",
        "    \"Krishna\", \"Arjuna\", \"Yudhishthira\", \"Bhima\", \"Nakula\", \"Sahadeva\",\n",
        "    \"Draupadi\", \"Duryodhana\", \"Dushasana\", \"Karna\", \"Drona\", \"Bhishma\",\n",
        "    \"Kunti\", \"Pandu\", \"Dhritarashtra\", \"Gandhari\", \"Vidura\", \"Shakuni\",\n",
        "    \"Abhimanyu\", \"Ghatotkacha\", \"Kripacharya\", \"Ashwatthama\", \"Jayadratha\"\n",
        "]\n",
        "\n",
        "MAHABHARAT_THEMES = [\n",
        "    \"dharma\", \"war\", \"duty\", \"honor\", \"betrayal\", \"sacrifice\",\n",
        "    \"friendship\", \"loyalty\", \"revenge\", \"justice\", \"wisdom\", \"courage\"\n",
        "]\n",
        "\n",
        "def extract_characters(text: str) -> List[str]:\n",
        "    \"\"\"Extract character names from text.\"\"\"\n",
        "    found_characters = []\n",
        "    text_lower = text.lower()\n",
        "    for char in MAHABHARAT_CHARACTERS:\n",
        "        if char.lower() in text_lower:\n",
        "            found_characters.append(char)\n",
        "    return found_characters\n",
        "\n",
        "def extract_themes(text: str) -> List[str]:\n",
        "    \"\"\"Extract themes from text.\"\"\"\n",
        "    found_themes = []\n",
        "    text_lower = text.lower()\n",
        "    for theme in MAHABHARAT_THEMES:\n",
        "        if theme.lower() in text_lower:\n",
        "            found_themes.append(theme)\n",
        "    return found_themes\n",
        "\n",
        "def create_text_chunks(chunks: List[str]) -> List[TextChunk]:\n",
        "    \"\"\"Convert text strings to TextChunk objects with metadata.\"\"\"\n",
        "    text_chunks = []\n",
        "    for i, text in enumerate(chunks):\n",
        "        chunk = TextChunk(\n",
        "            text=text,\n",
        "            chunk_id=f\"chunk_{i:04d}\",\n",
        "            characters=extract_characters(text),\n",
        "            themes=extract_themes(text),\n",
        "            chapter=None,\n",
        "            section=None\n",
        "        )\n",
        "        text_chunks.append(chunk)\n",
        "    return text_chunks\n",
        "\n",
        "# ============================================================================\n",
        "# VALIDATION AND DEDUPLICATION\n",
        "# ============================================================================\n",
        "\n",
        "def validate_chunk(text: str, min_length: int = 50, min_word_count: int = 5) -> bool:\n",
        "    \"\"\"Validate that a chunk is meaningful.\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return False\n",
        "    \n",
        "    text = text.strip()\n",
        "    \n",
        "    if len(text) < min_length:\n",
        "        return False\n",
        "    \n",
        "    words = text.split()\n",
        "    if len(words) < min_word_count:\n",
        "        return False\n",
        "    \n",
        "    alphanumeric_chars = sum(1 for c in text if c.isalnum())\n",
        "    if alphanumeric_chars < len(text) * 0.5:\n",
        "        return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "def deduplicate_chunks(chunks: List[TextChunk], similarity_threshold: float = 0.95) -> List[TextChunk]:\n",
        "    \"\"\"Remove duplicate and near-duplicate chunks.\"\"\"\n",
        "    seen_texts: Set[str] = set()\n",
        "    unique_chunks = []\n",
        "    \n",
        "    for chunk in chunks:\n",
        "        text = chunk.text.strip().lower()\n",
        "        \n",
        "        if text in seen_texts:\n",
        "            continue\n",
        "        \n",
        "        is_duplicate = False\n",
        "        words1 = set(text.split())\n",
        "        \n",
        "        for seen_text in seen_texts:\n",
        "            words2 = set(seen_text.split())\n",
        "            if len(words1) > 0 and len(words2) > 0:\n",
        "                intersection = len(words1 & words2)\n",
        "                union = len(words1 | words2)\n",
        "                similarity = intersection / union if union > 0 else 0\n",
        "                if similarity > similarity_threshold:\n",
        "                    is_duplicate = True\n",
        "                    break\n",
        "        \n",
        "        if not is_duplicate:\n",
        "            seen_texts.add(text)\n",
        "            unique_chunks.append(chunk)\n",
        "    \n",
        "    return unique_chunks\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
        "    \"\"\"Extract text from PDF file.\"\"\"\n",
        "    from pypdf import PdfReader\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    print(f\"Extracting text from PDF ({len(reader.pages)} pages)...\")\n",
        "    for i, page in enumerate(reader.pages):\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"  Processed {i + 1}/{len(reader.pages)} pages...\")\n",
        "    print(f\"Extracted {len(text)} characters from PDF\")\n",
        "    return text\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "# Check if vector DB already exists\n",
        "config_path = kaggle_dir / \"kaggle_config.yaml\"\n",
        "vector_store = KaggleVectorStore(config_path=config_path)\n",
        "\n",
        "if vector_store.count() > 0:\n",
        "    print(f\"\\n✓ Vector database already exists with {vector_store.count()} passages\")\n",
        "    print(\"Skipping PDF processing. If you want to rebuild, delete the vector_db directory first.\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CREATING VECTOR DATABASE FROM PDF\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    pdf_path = Path(PDF_PATH)\n",
        "    if not pdf_path.exists():\n",
        "        # Try alternative paths\n",
        "        alt_paths = [\n",
        "            Path(\"/kaggle/input/data/raw/Mahabharata.pdf\"),\n",
        "            Path(\"/kaggle/input/Mahabharata.pdf\"),\n",
        "        ]\n",
        "        for alt_path in alt_paths:\n",
        "            if alt_path.exists():\n",
        "                pdf_path = alt_path\n",
        "                break\n",
        "    \n",
        "    if not pdf_path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"PDF not found at {PDF_PATH}\\n\"\n",
        "            f\"Please upload Mahabharata.pdf to /kaggle/input/ first\"\n",
        "        )\n",
        "    \n",
        "    print(f\"\\nProcessing PDF: {pdf_path}\")\n",
        "    \n",
        "    # Step 1: Extract text from PDF\n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    print(\"STEP 1: Extracting text from PDF\")\n",
        "    print(\"-\"*40)\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    original_length = len(text)\n",
        "    \n",
        "    # Step 2: Clean text (preserving paragraph structure)\n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    print(\"STEP 2: Cleaning text (preserving paragraphs)\")\n",
        "    print(\"-\"*40)\n",
        "    text = clean_text(text)\n",
        "    cleaned_length = len(text)\n",
        "    print(f\"  Cleaned: {original_length:,} → {cleaned_length:,} chars ({100*cleaned_length/original_length:.1f}% retained)\")\n",
        "    \n",
        "    # Count paragraphs for info\n",
        "    paragraph_count = len(split_into_paragraphs(text))\n",
        "    print(f\"  Detected {paragraph_count} paragraphs\")\n",
        "    \n",
        "    # Step 3: Initialize embedder (needed for semantic chunking)\n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    print(\"STEP 3: Initializing embedder\")\n",
        "    print(\"-\"*40)\n",
        "    embedder = KaggleEmbedder(config_path=config_path)\n",
        "    print(f\"  Model: {embedder.model_name}\")\n",
        "    \n",
        "    # Step 4: Hybrid semantic + paragraph-aware chunking\n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    print(\"STEP 4: Hybrid semantic chunking\")\n",
        "    print(\"-\"*40)\n",
        "    print(f\"  Config: target={CHUNK_CONFIG['target_size']}, min={CHUNK_CONFIG['min_size']}, max={CHUNK_CONFIG['max_size']}\")\n",
        "    print(f\"  Semantic threshold: {CHUNK_CONFIG['semantic_threshold']}\")\n",
        "    \n",
        "    chunk_texts = hybrid_semantic_chunking(\n",
        "        text, \n",
        "        embedder, \n",
        "        config=CHUNK_CONFIG,\n",
        "        add_overlap=True\n",
        "    )\n",
        "    \n",
        "    # Step 5: Create TextChunk objects with metadata\n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    print(\"STEP 5: Extracting metadata\")\n",
        "    print(\"-\"*40)\n",
        "    chunks = create_text_chunks(chunk_texts)\n",
        "    print(f\"  Created {len(chunks)} chunks with metadata\")\n",
        "    \n",
        "    # Count metadata\n",
        "    chunks_with_characters = sum(1 for c in chunks if c.characters)\n",
        "    chunks_with_themes = sum(1 for c in chunks if c.themes)\n",
        "    print(f\"  Chunks with characters: {chunks_with_characters}\")\n",
        "    print(f\"  Chunks with themes: {chunks_with_themes}\")\n",
        "    \n",
        "    # Step 6: Validate chunks\n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    print(\"STEP 6: Validating chunks\")\n",
        "    print(\"-\"*40)\n",
        "    valid_chunks = [chunk for chunk in chunks if validate_chunk(chunk.text)]\n",
        "    invalid_count = len(chunks) - len(valid_chunks)\n",
        "    print(f\"  Valid chunks: {len(valid_chunks)} (removed {invalid_count} invalid)\")\n",
        "    \n",
        "    # Step 7: Deduplicate chunks\n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    print(\"STEP 7: Deduplicating chunks\")\n",
        "    print(\"-\"*40)\n",
        "    unique_chunks = deduplicate_chunks(valid_chunks)\n",
        "    duplicate_count = len(valid_chunks) - len(unique_chunks)\n",
        "    print(f\"  Unique chunks: {len(unique_chunks)} (removed {duplicate_count} duplicates)\")\n",
        "    \n",
        "    # Step 8: Generate final embeddings\n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    print(\"STEP 8: Generating final embeddings\")\n",
        "    print(\"-\"*40)\n",
        "    texts = [chunk.text for chunk in unique_chunks]\n",
        "    embeddings = embedder.embed(texts)\n",
        "    print(f\"  Generated {len(embeddings)} embeddings\")\n",
        "    \n",
        "    # Step 9: Add to vector store\n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    print(\"STEP 9: Adding passages to vector database\")\n",
        "    print(\"-\"*40)\n",
        "    passages = []\n",
        "    for i, chunk in enumerate(unique_chunks):\n",
        "        passage = Passage(\n",
        "            chunk_id=chunk.chunk_id,\n",
        "            text=chunk.text,\n",
        "            embedding=embeddings[i],\n",
        "            characters=chunk.characters,\n",
        "            themes=chunk.themes,\n",
        "            chapter=chunk.chapter,\n",
        "            section=chunk.section\n",
        "        )\n",
        "        passages.append(passage)\n",
        "    \n",
        "    # Add in batches\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(passages), batch_size):\n",
        "        batch = passages[i:i + batch_size]\n",
        "        vector_store.add_passages(batch)\n",
        "        print(f\"  Added batch {i//batch_size + 1}/{(len(passages) + batch_size - 1)//batch_size}\")\n",
        "    \n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"VECTOR DATABASE CREATION COMPLETE\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"  Source paragraphs: {paragraph_count}\")\n",
        "    print(f\"  Initial chunks (after semantic): {len(chunks)}\")\n",
        "    print(f\"  Invalid removed: {invalid_count}\")\n",
        "    print(f\"  Duplicates removed: {duplicate_count}\")\n",
        "    print(f\"  Final passages: {vector_store.count()}\")\n",
        "    \n",
        "    # Chunk size statistics\n",
        "    chunk_sizes = [len(c.text) for c in unique_chunks]\n",
        "    print(f\"\\n  Chunk size stats:\")\n",
        "    print(f\"    Min: {min(chunk_sizes)} chars\")\n",
        "    print(f\"    Max: {max(chunk_sizes)} chars\")\n",
        "    print(f\"    Avg: {sum(chunk_sizes) // len(chunk_sizes)} chars\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generate Story Scripts and Videos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Story Scripts and Videos\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "sys.path.insert(0, str(kaggle_dir))\n",
        "from story_pipeline import (\n",
        "    KaggleVectorStore,\n",
        "    KaggleRetriever,\n",
        "    KaggleStoryGenerator,\n",
        "    KaggleNarrationGenerator\n",
        ")\n",
        "from wan21_generator import Wan21KaggleGenerator\n",
        "\n",
        "config_path = kaggle_dir / \"kaggle_config.yaml\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 1: LOADING VECTOR DATABASE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load vector store\n",
        "vector_store = KaggleVectorStore(config_path=config_path)\n",
        "print(f\"Vector DB loaded: {vector_store.count()} passages\")\n",
        "\n",
        "if vector_store.count() == 0:\n",
        "    print(\"\\n⚠ WARNING: Vector database is empty!\")\n",
        "    print(\"Please run the previous cell to create vector DB from PDF first.\")\n",
        "    raise ValueError(\"Vector database is empty\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 2: RETRIEVING PASSAGES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize retriever\n",
        "retriever = KaggleRetriever(vector_store=vector_store, config_path=config_path)\n",
        "\n",
        "# Retrieve seed passage\n",
        "seed_passages = retriever.retrieve_diverse(query=QUERY, n_results=1)\n",
        "if not seed_passages:\n",
        "    raise ValueError(\"Failed to retrieve seed passage\")\n",
        "\n",
        "seed_passage = seed_passages[0]\n",
        "print(f\"\\nSeed Passage ID: {seed_passage['id']}\")\n",
        "print(f\"Text preview: {seed_passage['text'][:200]}...\")\n",
        "\n",
        "# Retrieve context passages\n",
        "context_passages = retriever.retrieve_context(seed_passage, n_context=3)\n",
        "print(f\"\\nRetrieved {len(context_passages)} context passages\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 3: GENERATING STORY SCRIPTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Generate story\n",
        "story_generator = KaggleStoryGenerator(config_path=config_path)\n",
        "\n",
        "story_data = story_generator.generate_story(\n",
        "    seed_passage=seed_passage['text'],\n",
        "    context_passages=[p['text'] for p in context_passages]\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Generated story with {len(story_data.get('story_sequence', []))} scenes\")\n",
        "\n",
        "# Generate narration\n",
        "narration_generator = KaggleNarrationGenerator(config_path=config_path)\n",
        "narration_data = narration_generator.generate_narration(story_data)\n",
        "\n",
        "# Save generated scripts\n",
        "output_dir = Path(\"/kaggle/working/output\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Convert to video_prompts format\n",
        "video_prompts = []\n",
        "for scene in story_data.get('story_sequence', []):\n",
        "    clip_prompt = scene.get('clip_prompt', {})\n",
        "    video_prompts.append({\n",
        "        \"scene_number\": scene.get('scene_number', len(video_prompts) + 1),\n",
        "        \"title\": scene.get('title', f\"Scene {scene.get('scene_number', len(video_prompts) + 1)}\"),\n",
        "        \"video_prompt\": clip_prompt.get('visual_description', ''),\n",
        "        \"camera_angles\": clip_prompt.get('camera_angles', ''),\n",
        "        \"key_objects\": clip_prompt.get('key_objects', ''),\n",
        "        \"ambient_audio\": clip_prompt.get('ambient_audio', ''),\n",
        "        \"duration_seconds\": 5\n",
        "    })\n",
        "\n",
        "# Save video prompts\n",
        "prompts_file = output_dir / f\"video_prompts_{timestamp}.json\"\n",
        "with open(prompts_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(video_prompts, f, indent=2, ensure_ascii=False)\n",
        "print(f\"\\n✓ Saved video prompts: {prompts_file}\")\n",
        "\n",
        "# Save narration\n",
        "narration_file = output_dir / f\"narration_{timestamp}.json\"\n",
        "with open(narration_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(narration_data, f, indent=2, ensure_ascii=False)\n",
        "print(f\"✓ Saved narration: {narration_file}\")\n",
        "\n",
        "# Display the generated scripts\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATED VIDEO PROMPTS\")\n",
        "print(\"=\"*60)\n",
        "for scene in video_prompts:\n",
        "    print(f\"\\nScene {scene['scene_number']}: {scene['title']}\")\n",
        "    print(f\"  Prompt: {scene['video_prompt'][:150]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATED NARRATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Title: {narration_data['story_title']}\")\n",
        "print(f\"\\nHook: {narration_data['hook']}\")\n",
        "print(f\"\\nStory: {narration_data['story'][:200]}...\")\n",
        "print(f\"\\nCTA: {narration_data['cta']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 4: GENERATING VIDEOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize video generator\n",
        "video_generator = Wan21KaggleGenerator(config_path=config_path)\n",
        "\n",
        "# Generate videos from scenes\n",
        "video_paths = video_generator.generate_from_scenes(video_prompts)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nGenerated {len(video_paths)} videos:\")\n",
        "for path in video_paths:\n",
        "    print(f\"  - {path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zip output folder for easy download\n",
        "import shutil\n",
        "\n",
        "output_dir = Path(\"/kaggle/working/output\")\n",
        "zip_path = Path(\"/kaggle/working/videos_output.zip\")\n",
        "\n",
        "if output_dir.exists() and any(output_dir.iterdir()):\n",
        "    shutil.make_archive(str(zip_path).replace('.zip', ''), 'zip', output_dir)\n",
        "    print(f\"✓ Created zip file: {zip_path}\")\n",
        "    print(f\"  Size: {zip_path.stat().st_size / (1024*1024):.2f} MB\")\n",
        "else:\n",
        "    print(\"No output files to zip\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
